{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 50000\neval_interval = max_iters//20\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# write function that takes a path of a directory : returns a list of all the paths of the files in that directory\nimport os\n\ndef getFiles(path):\n    files = []\n    for r, d, f in os.walk(path):\n        for file in f:\n            files.append(os.path.join(r, file))\n    return files\n#print(getFiles(\"./data\"))\n\n# store them in a list\npath_txt_files = getFiles(\"/kaggle/input/txtfilesdialoguesknowledge/chomskyProj\")\npath_txt_files += getFiles(\"/kaggle/input/tinyshakespearetxt/input.txt\")\n# read the .txt files and save them in one 'text' variable\ntext = ''\nfor path_txt_file in path_txt_files:\n    with open(path_txt_file, 'r') as f:\n        text += f.read()\n        # add a point and space an between each text\n        text += '. '\n        \n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(\"vocab size:\", vocab_size)\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\nencoded_text = encode(text)\nprint(\"total number of tokens :\", len(encoded_text))\ndata = torch.tensor(encoded_text, dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-15T15:36:59.387129Z","iopub.execute_input":"2023-06-15T15:36:59.387509Z","iopub.status.idle":"2023-06-15T15:37:00.009495Z","shell.execute_reply.started":"2023-06-15T15:36:59.387465Z","shell.execute_reply":"2023-06-15T15:37:00.008490Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"vocab size: 107\ntotal number of tokens : 2231292\n0.215147 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"for iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        \n        # generate from the model\n        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n        print(decode(m.generate(context, max_new_tokens=30)[0].tolist()), '\\n---\\n')\n        \n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2023-06-15T15:37:04.465194Z","iopub.execute_input":"2023-06-15T15:37:04.465561Z","iopub.status.idle":"2023-06-15T15:54:42.764263Z","shell.execute_reply.started":"2023-06-15T15:37:04.465530Z","shell.execute_reply":"2023-06-15T15:54:42.763110Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"step 0: train loss 4.9123, val loss 4.9162\n\nj—rpeitUo—ê;dM$du%JjU>9iD–f5a8 \n---\n\nstep 1666: train loss 1.8376, val loss 1.8671\n\n\n\nit littelf it their agow to  \n---\n\nstep 3332: train loss 1.6668, val loss 1.7321\n\n\nthat's I likew ibding of and  \n---\n\nstep 4998: train loss 1.5812, val loss 1.6580\n\n\nsaid compledived for there be \n---\n\nstep 6664: train loss 1.5385, val loss 1.6281\n\ngo over most\npast can form vie \n---\n\nstep 8330: train loss 1.5077, val loss 1.6059\n\n\nundertical Section walk to ou \n---\n\nstep 9996: train loss 1.5056, val loss 1.5835\n\nand toc as reases  one thonics \n---\n\nstep 11662: train loss 1.4735, val loss 1.5717\n\n\ntrutning retive mackin more c \n---\n\nstep 13328: train loss 1.4598, val loss 1.5681\n\nficame, of the dafferents\n\nof  \n---\n\nstep 14994: train loss 1.4517, val loss 1.5553\n\nthe new that something that th \n---\n\nstep 16660: train loss 1.4430, val loss 1.5402\n\n\nstites you all the onomor\nins \n---\n\nstep 18326: train loss 1.4290, val loss 1.5405\n\newein in the classes those, th \n---\n\nstep 19992: train loss 1.4270, val loss 1.5219\n\n\nAND HE INL]\n\nI P i HIM, SOGEW \n---\n\nstep 21658: train loss 1.4231, val loss 1.5232\n\nthat are cientific powerful I  \n---\n\nstep 23324: train loss 1.4046, val loss 1.5186\n\nChirason, exposelies.\n\nWell I' \n---\n\nstep 24990: train loss 1.4034, val loss 1.5215\n\n\nwe'll breaking hamber now all \n---\n\nstep 26656: train loss 1.4035, val loss 1.5133\n\nthat anything 202 solution.\n\nA \n---\n\nstep 28322: train loss 1.3968, val loss 1.5082\n\nso where lowment that  now ele \n---\n\nstep 29988: train loss 1.3875, val loss 1.5033\n\nuse the first are not past I,  \n---\n\nstep 31654: train loss 1.3923, val loss 1.5017\n\n\nit business are afternoon the \n---\n\nstep 33320: train loss 1.3775, val loss 1.5000\n\nthe crimatistion, but but mach \n---\n\nstep 34986: train loss 1.3739, val loss 1.5042\n\n\nat MachiavelligHt?\n\nLike, now \n---\n\nstep 36652: train loss 1.3825, val loss 1.5100\n\nthe Neation of a very cervers\n \n---\n\nstep 38318: train loss 1.3729, val loss 1.5003\n\n\nhas simply important with us  \n---\n\nstep 39984: train loss 1.3771, val loss 1.5050\n\n\nthere's a lot moviebute right \n---\n\nstep 41650: train loss 1.3713, val loss 1.4940\n\nenvirormats is not just consti \n---\n\nstep 43316: train loss 1.3709, val loss 1.4913\n\ndon't understand to me interes \n---\n\nstep 44982: train loss 1.3676, val loss 1.4964\n\nssmellenger there the big\n\nenc \n---\n\nstep 46648: train loss 1.3640, val loss 1.4871\n\nShautification the basic of co \n---\n\nstep 48314: train loss 1.3738, val loss 1.4912\n\nthe freedom is what it was a m \n---\n\nstep 49980: train loss 1.3566, val loss 1.4920\n\nreveplacene dopinal scarious\n\n \n---\n\nstep 49999: train loss 1.3586, val loss 1.4963\n\n\nthat is a very union,\n\nperhap \n---\n\n\nimpactant the\n\nuniversal, b, freedom more?\n\nIf you think which that we we just you\n\njust nature is rational alvemet is that\n\nthat nuclearing is that mostivism cances\n\ncurve uh uh modern the economics on 60\n\nAI I houh was that case how so\n\nwell the pand and this extremul up and today in the proact with ufferticip\n\ntabils okay what I was a company\n\nof men the tit unerretaintary writing and he over version\n\nbut you feel that becomes\n\nthis tagms that are human normal imposses, and\n\nbut you don't think and it's this news\n\nsupposed being a\n\nrupple with the book on the haitten,\n\nand on the chincerned to and on.\n\nSo In the understand\n\nclic seriously that to hasn't\n\nsocial pournits and to with book at the right now I think it's\n\ncareer too you and one become\n\nfaht is that we can't be water\n\nof the intellectual want\nwhat are had and and I have to chose nothing becuro we will advice that\n\nare I was going to presurity and for a famous and then\n\nthey had this biggers like the AB hunterchance\n\nand so versions I way should I have to\n\n\ngot a, Origin's deal, while\nmay force from outside\n\nbut in I a thinking one act and you\n\npart is you thinking of thought\n\nwe very rever about use than thought this\n\nmorning.\n\nAnd jornal say that the space now of\n\nfor people the way it's the heart right\n\nthat because you back him book a\n\nfactor of the proof thing that we\n\nhave alreat moment that they replaus very possibly prejuction knows\n\nwell me I'm thinking if there okay a who\n\npre-anding on hundred for ever go question on\n\na\n\ndemocratic part of worlds there\n\nare kind of of the annouss what I was this\n\ndoesn't extremely but he you could be\n\nlived at the, the fact that just a\n\nconcern world's woring and and that the\n\nquestion of industriating you chauge,\n\nalmost these threat the event to\n\nused to public to have a far true\n\nI didn't see it in a\n\ntax ordinary technology whosage and uh\n\nare contentions that was presectional nakful so I think about that is\n\num zero care but the etc.\n\nYou know that's the\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}